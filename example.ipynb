{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray End-to-End NLP Example\n",
    "\n",
    "**GOAL:** In this example, we will go through how to use Ray to implement an end-to-end NLP example. Specifically, we will go through:\n",
    "\n",
    "- How to use RaySGD to scale the training of HuggingFace Transformer library.\n",
    "- How to serve the trained model with Ray Serve\n",
    "\n",
    "First we install some dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install uvicorn\n",
    "# !pip install blist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also import the libraries needed for the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "from filelock import FileLock\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "from ray.util.sgd.torch import TrainingOperator\n",
    "from ray.util.sgd import TorchTrainer\n",
    "\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import (DataLoader, RandomSampler, \n",
    "                              SequentialSampler, TensorDataset)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer,\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    LineByLineTextDataset,\n",
    "    PreTrainedTokenizer,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "except ImportError:\n",
    "    amp = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also initialize Ray to use RaySGD and Ray Serve later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.31.30.37',\n",
       " 'raylet_ip_address': '172.31.30.37',\n",
       " 'redis_address': '172.31.30.37:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-06-05_22-20-41_529898_32512/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-06-05_22-20-41_529898_32512/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-06-05_22-20-41_529898_32512'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(address=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Download the dataset. Here we use the wikitext-2 dataset as a demonstrative example. Any text datasets are feasible for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-05 22:32:11--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.40.150\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.40.150|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4475746 (4.3M) [application/zip]\n",
      "Saving to: ‘wikitext-2-v1.zip.1’\n",
      "\n",
      "wikitext-2-v1.zip.1 100%[===================>]   4.27M  6.45MB/s    in 0.7s    \n",
      "\n",
      "2020-06-05 22:32:12 (6.45 MB/s) - ‘wikitext-2-v1.zip.1’ saved [4475746/4475746]\n",
      "\n",
      "Archive:  wikitext-2-v1.zip\n",
      "   creating: wikitext-2/\n",
      "  inflating: wikitext-2/wiki.test.tokens  \n",
      "  inflating: wikitext-2/wiki.valid.tokens  \n",
      "  inflating: wikitext-2/wiki.train.tokens  \n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
    "!unzip wikitext-2-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Training with Ray SGD\n",
    "\n",
    "In this section, we show how to use RaySGD to scale up the training of the HuggingFace Transformer library.\n",
    "\n",
    "First we define the arguments for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(adam_epsilon=1e-08, address='auto', block_size=128, cache_dir=None, config_name=None, dataloader_drop_last=False, device=device(type='cuda'), do_eval=True, do_predict=False, do_train=True, eval_data_file='/home/ubuntu/ray-e2e-nlp-example/wikitext-2/wiki.test.tokens', evaluate_during_training=False, fp16=True, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, line_by_line=False, local_rank=-1, logging_dir='runs/Jun05_22-32-13_ip-172-31-30-37', logging_first_step=False, logging_steps=500, max_grad_norm=1.0, max_steps=-1, model_name_or_path='gpt2', model_type='gpt2', no_cuda=False, num_train_epochs=3, num_workers=4, output_dir='/home/ubuntu/ray-e2e-nlp-example/output_dir/', overwrite_cache=False, overwrite_output_dir=False, per_device_eval_batch_size=8, per_device_train_batch_size=8, per_gpu_eval_batch_size=None, per_gpu_train_batch_size=None, save_steps=500, save_total_limit=None, seed=42, tensorboard_dir='/home/ubuntu/ray_results/ray-e2e-nlp-example/', tokenizer_name=None, tpu_metrics_debug=False, tpu_num_cores=None, train_data_file='/home/ubuntu/ray-e2e-nlp-example/wikitext-2/wiki.train.tokens', warmup_steps=0, weight_decay=0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments (from hugging face)\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"/home/ubuntu/ray-e2e-nlp-example/output_dir/\",\n",
    "    learning_rate = 2e-5,\n",
    "    num_train_epochs = 3,\n",
    "    fp16 = True,\n",
    "    do_train = True,\n",
    "    do_eval = True\n",
    ")\n",
    "args = argparse.Namespace(**vars(training_arguments))\n",
    "# args = training_arguments\n",
    "\n",
    "# Model arguments\n",
    "args.model_name_or_path = \"gpt2\"\n",
    "args.model_type = \"gpt2\"\n",
    "args.config_name = None\n",
    "args.tokenizer_name = None\n",
    "args.cache_dir = None\n",
    "\n",
    "# Data processing arguments\n",
    "args.train_data_file = \"/home/ubuntu/ray-e2e-nlp-example/wikitext-2/wiki.train.tokens\"\n",
    "args.eval_data_file = \"/home/ubuntu/ray-e2e-nlp-example/wikitext-2/wiki.test.tokens\"\n",
    "args.line_by_line = False\n",
    "args.block_size = 128\n",
    "args.overwrite_cache = False\n",
    "args.tensorboard_dir = \"/home/ubuntu/ray_results/ray-e2e-nlp-example/\"\n",
    "\n",
    "# Ray arguments\n",
    "args.num_workers = 4\n",
    "args.address = \"auto\"\n",
    "\n",
    "use_gpu = torch.cuda.is_available() and not args.no_cuda\n",
    "args.device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the random seeds for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "set_seed(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creator\n",
    "\n",
    "Then we define the data creator for the trainer. The data creator creates a train data loader object for training. Note that we do not need to wrap the data loader with a distributed loader since the RaySGD trainer will automatically does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_creator(config):\n",
    "    args = config[\"args\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.tokenizer_name\n",
    "        if args.tokenizer_name else args.model_name_or_path,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "    \n",
    "    if args.block_size <= 0:\n",
    "        args.block_size = tokenizer.max_len\n",
    "        # Our input block size will be the max possible for the model\n",
    "    else:\n",
    "        args.block_size = min(args.block_size, tokenizer.max_len)\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer, file_path=args.train_data_file, \n",
    "        block_size=args.block_size, overwrite_cache=args.overwrite_cache\n",
    "    )\n",
    "    train_sampler = RandomSampler(train_dataset) if not dist.is_initialized() else None\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creator\n",
    "\n",
    "The model creator creates models for each training worker. Here we initialize the modelwith a trained GPT-2 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creator(config):\n",
    "    with FileLock(os.path.expanduser(\"~/.download.lock\")):\n",
    "        args = config[\"args\"]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            args.tokenizer_name\n",
    "            if args.tokenizer_name else args.model_name_or_path,\n",
    "            cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "        )\n",
    "        model_config = AutoConfig.from_pretrained(\n",
    "            args.config_name if args.config_name else args.model_name_or_path,\n",
    "            cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "        )\n",
    "        model = AutoModelWithLMHead.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "            config=model_config,\n",
    "            cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "        )\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer creator\n",
    "\n",
    "We use Adam optimizer for training. In the following code, we group the parameters into two groups: one with weight decay and one without weight decay for training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_creator(model, config):\n",
    "    args = config[\"args\"]\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=args.learning_rate,\n",
    "        eps=args.adam_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training operator\n",
    "\n",
    "Next we define the training operator. The training operator defines a custom training loop that includes gradient accumulation (i.e. perform gradient updates after a certain amount of forward and backward propagations). The training operator here also defines the warmup learning rate scheduler for the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def announce_training(args, dataset_len, t_total):\n",
    "    # Train!\n",
    "    print(\"***** Running training *****\")\n",
    "    print(\"CUDA_VISIBLE_DEVICES\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "    print(\"  Num examples = %d\" % dataset_len)\n",
    "    print(\"  Num Epochs = %d\" % args.num_train_epochs)\n",
    "    print(\"  Instantaneous batch size per GPU = %d\" %\n",
    "          args.per_device_train_batch_size)\n",
    "    print(\n",
    "        \"  Total train batch size (w. parallel, distributed & accum) = %d\" %\n",
    "        args.per_device_train_batch_size * args.gradient_accumulation_steps *\n",
    "        args.num_workers\n",
    "    )\n",
    "    print(\"  Gradient Accumulation steps = %d\" %\n",
    "          args.gradient_accumulation_steps)\n",
    "    print(\"  Total optimization steps = %d\" % t_total)\n",
    "\n",
    "\n",
    "class TransformerOperator(TrainingOperator):\n",
    "    def setup(self, config):\n",
    "        self.args = args = config[\"args\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            args.tokenizer_name\n",
    "            if args.tokenizer_name else args.model_name_or_path,\n",
    "            cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "        )\n",
    "\n",
    "        self.train_data_len = len(self.train_loader)\n",
    "        self._warmup_scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=args.warmup_steps,\n",
    "            num_training_steps=self.calculate_t_total())\n",
    "        self._global_step = 0\n",
    "\n",
    "        announce_training(args, self.train_data_len, self.calculate_t_total())\n",
    "\n",
    "    def train_batch(self, batch, batch_info=None):\n",
    "        args = self.args\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "        step = batch_info[\"batch_idx\"]\n",
    "\n",
    "        model.train()\n",
    "        batch = batch.to(self.device)\n",
    "        outputs = model(input_ids=batch, labels=batch)\n",
    "\n",
    "        # model outputs are always tuple in transformers (see doc)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "        if args.fp16:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "\n",
    "        # last step in epoch but step is always smaller\n",
    "        # than gradient_accumulation_steps\n",
    "        ending = (self.train_data_len <= args.gradient_accumulation_steps\n",
    "                  and (step + 1) == self.train_data_len)\n",
    "        if (step + 1) % args.gradient_accumulation_steps == 0 or ending:\n",
    "            if args.fp16:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    amp.master_params(optimizer), args.max_grad_norm)\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                               args.max_grad_norm)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self._warmup_scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            self._global_step += 1\n",
    "\n",
    "        learning_rate_scalar = self._warmup_scheduler.get_lr()[0]\n",
    "        return {\"learning_rate\": learning_rate_scalar, \"loss\": batch_loss}\n",
    "\n",
    "    def calculate_t_total(self):\n",
    "        args = self.args\n",
    "        grad_accum_steps = args.gradient_accumulation_steps\n",
    "        train_data_len = len(self.train_loader)\n",
    "        if args.max_steps > 0:\n",
    "            t_total = args.max_steps\n",
    "            args.num_train_epochs = args.max_steps // (\n",
    "                train_data_len // grad_accum_steps) + 1\n",
    "        else:\n",
    "            t_total = (\n",
    "                train_data_len // grad_accum_steps * args.num_train_epochs)\n",
    "        return t_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RaySGD Torch Trainer\n",
    "\n",
    "Finallly we define a RaySGD Torch trainer to perform distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438dd5c59324456e903c0d5537007a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34caff8aaea745ff80f4f2705d4bf0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112e439ed33947798458282de04de98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d897094ec6c24409a6847d79455063f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m Defaults for this optimization level are:\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m enabled                : True\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m opt_level              : O1\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m cast_model_type        : None\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m patch_torch_functions  : True\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m keep_batchnorm_fp32    : None\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m master_weights         : None\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m loss_scale             : dynamic\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m Processing user overrides (additional kwargs that are not None)...\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m After processing overrides, optimization options are:\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m enabled                : True\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m opt_level              : O1\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m cast_model_type        : None\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m patch_torch_functions  : True\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m keep_batchnorm_fp32    : None\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m master_weights         : None\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m loss_scale             : dynamic\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m Defaults for this optimization level are:\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m enabled                : True\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m opt_level              : O1\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m cast_model_type        : None\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m patch_torch_functions  : True\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m keep_batchnorm_fp32    : None\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m master_weights         : None\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m loss_scale             : dynamic\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m Processing user overrides (additional kwargs that are not None)...\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m After processing overrides, optimization options are:\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m enabled                : True\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m opt_level              : O1\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m cast_model_type        : None\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m patch_torch_functions  : True\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m keep_batchnorm_fp32    : None\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m master_weights         : None\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m loss_scale             : dynamic\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m Defaults for this optimization level are:\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m enabled                : True\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m opt_level              : O1\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m cast_model_type        : None\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m patch_torch_functions  : True\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m keep_batchnorm_fp32    : None\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m master_weights         : None\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m loss_scale             : dynamic\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m Processing user overrides (additional kwargs that are not None)...\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m After processing overrides, optimization options are:\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m enabled                : True\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m opt_level              : O1\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m cast_model_type        : None\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m patch_torch_functions  : True\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m keep_batchnorm_fp32    : None\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m master_weights         : None\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m loss_scale             : dynamic\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m ***** Running training *****\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m CUDA_VISIBLE_DEVICES 2\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m   Num examples = 598\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m   Num Epochs = 3\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m   Instantaneous batch size per GPU = 8\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m   Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m   Gradient Accumulation steps = 1\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m   Total optimization steps = 1794\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m ***** Running training *****\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m CUDA_VISIBLE_DEVICES 1\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m   Num examples = 598\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m   Num Epochs = 3\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m   Instantaneous batch size per GPU = 8\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m   Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m   Gradient Accumulation steps = 1\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m   Total optimization steps = 1794\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m ***** Running training *****\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m CUDA_VISIBLE_DEVICES 0\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m   Num examples = 598\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m   Num Epochs = 3\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m   Instantaneous batch size per GPU = 8\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m   Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m   Gradient Accumulation steps = 1\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m   Total optimization steps = 1794\n",
      "***** Running training *****\n",
      "CUDA_VISIBLE_DEVICES 3\n",
      "  Num examples = 598\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per GPU = 8\n",
      "  Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8  Total train batch size (w. parallel, distributed & accum) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1794\n"
     ]
    }
   ],
   "source": [
    "trainer = TorchTrainer(\n",
    "    model_creator=model_creator,\n",
    "    data_creator=data_creator,\n",
    "    optimizer_creator=optimizer_creator,\n",
    "    training_operator_cls=TransformerOperator,\n",
    "    use_fp16=args.fp16,\n",
    "    apex_args={\"opt_level\": args.fp16_opt_level},\n",
    "    num_workers=args.num_workers,\n",
    "    use_gpu=use_gpu,\n",
    "    use_tqdm=False,\n",
    "    config={\"args\": args}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Here we define the evalutate function, which evaluates the trained model on the evalutation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    eval_dataset = TextDataset(\n",
    "        tokenizer=tokenizer, file_path=args.eval_data_file, \n",
    "        block_size=args.block_size, overwrite_cache=args.overwrite_cache\n",
    "    )\n",
    "\n",
    "    args.eval_batch_size = args.per_device_eval_batch_size\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        sampler=eval_sampler,\n",
    "        batch_size=args.eval_batch_size)\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = batch.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch, labels=batch)\n",
    "            tmp_eval_loss = outputs[0]\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    return {\"loss\": eval_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "We define the training loop here. We will evaluate the model on the validation set every epoch. We also log the results to the tensorboard and thus we can get the training curve by clicking the tensorboard button on the Anyscale dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "\u001b[2m\u001b[36m(pid=33032)\u001b[0m Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "\u001b[2m\u001b[36m(pid=33029)\u001b[0m Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "\u001b[2m\u001b[36m(pid=33058)\u001b[0m Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Training stats: {'num_samples': 2392, 'epoch': 1, 'batch_count': 598, 'learning_rate': 1.6661092530657763e-05, 'last_learning_rate': 1.3333333333333333e-05, 'loss': 3.389202869456747, 'last_loss': 3.241966962814331}\n",
      "Validation stats: {'loss': 2.9518456591041855}\n",
      "Training stats: {'num_samples': 2392, 'epoch': 2, 'batch_count': 598, 'learning_rate': 9.994425863991069e-06, 'last_learning_rate': 6.666666666666667e-06, 'loss': 3.210772928984269, 'last_loss': 3.298543930053711}\n",
      "Validation stats: {'loss': 2.9231047490063835}\n",
      "Training stats: {'num_samples': 2392, 'epoch': 3, 'batch_count': 598, 'learning_rate': 3.3277591973244156e-06, 'last_learning_rate': 0.0, 'loss': 3.161288238289364, 'last_loss': 3.2020671367645264}\n",
      "Validation stats: {'loss': 2.9164522288167354}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = trainer.get_local_operator().tokenizer\n",
    "local_model = trainer.get_model()\n",
    "\n",
    "epochs_trained = 0\n",
    "train_iterator = range(\n",
    "    epochs_trained,\n",
    "    int(args.num_train_epochs)\n",
    ")\n",
    "tensorboard_writer = SummaryWriter(log_dir=args.tensorboard_dir, flush_secs=30)\n",
    "\n",
    "if args.do_train:\n",
    "    for _ in train_iterator:\n",
    "        train_stats = trainer.train()\n",
    "        eval_stats = evaluate(args, local_model, tokenizer)\n",
    "        print(\"Training stats:\", train_stats)\n",
    "        print(\"Validation stats:\", eval_stats)\n",
    "        tensorboard_writer.add_scalar('Loss/train', train_stats['loss'], train_stats[\"epoch\"])\n",
    "        tensorboard_writer.add_scalar('Loss/eval', eval_stats['loss'], train_stats[\"epoch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training finishes, we save the model to the disk and also shutdown the trainer to release the GPUs for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/ubuntu/ray-e2e-nlp-example/output_dir/\n"
     ]
    }
   ],
   "source": [
    "def save_model(args, model, tokenizer):\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    print(\"Saving model checkpoint to %s\" % args.output_dir)\n",
    "    model.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "save_model(args, local_model, tokenizer)\n",
    "trainer.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving\n",
    "\n",
    "Here we demonstrate how to use Ray Serve to serve the model we just trained.\n",
    "\n",
    "First we define a serving backend, which is a Ray actor that processes incoming requests. Here we assume the request is a prefix of an English sentence, and we will use our model to predict the next word of the input segement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33037)\u001b[0m 2020-06-05 22:57:38,205\tINFO master.py:183 -- Starting metric exporter with name 'SERVE_METRIC_SINK_ACTOR'\n",
      "\u001b[2m\u001b[36m(pid=33037)\u001b[0m 2020-06-05 22:57:38,222\tINFO master.py:129 -- Starting router with name 'SERVE_ROUTER_ACTOR'\n",
      "\u001b[2m\u001b[36m(pid=33037)\u001b[0m 2020-06-05 22:57:38,233\tINFO master.py:152 -- Starting HTTP proxy with name 'SERVE_PROXY_ACTOR' on node 'node:172.31.30.37'\n",
      "\u001b[2m\u001b[36m(pid=33046)\u001b[0m INFO:     Started server process [33046]\n",
      "\u001b[2m\u001b[36m(pid=33046)\u001b[0m INFO:     Waiting for application startup.\n",
      "\u001b[2m\u001b[36m(pid=33046)\u001b[0m INFO:     Application startup complete.\n"
     ]
    }
   ],
   "source": [
    "serve.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWord:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        self.model.to(args.device)\n",
    "\n",
    "    def __call__(self, flask_request):\n",
    "        input_sentence = flask_request.data.decode(\"utf-8\")\n",
    "        generated = self.tokenizer.encode(input_sentence)\n",
    "        context = torch.tensor([generated]).to(args.device)\n",
    "        past = None\n",
    "\n",
    "        output, past = self.model(context, past=past)\n",
    "        token = torch.argmax(output[..., -1, :])\n",
    "\n",
    "        generated += [token.tolist()]\n",
    "        context = token.unsqueeze(0)\n",
    "\n",
    "        sequence = self.tokenizer.decode(generated)\n",
    "\n",
    "        return sequence\n",
    "\n",
    "# If the backend name have been defined before, we should delete them before create a new one.\n",
    "# serve.delete_endpoint(\"nextword\")\n",
    "serve.create_backend(\"nextword\", NextWord, args, ray_actor_options={\"num_gpus\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a serving endpoint at `/nextword`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33037)\u001b[0m 2020-06-05 22:57:53,092\tINFO master.py:536 -- Registering route /nextword to endpoint nextword with methods ['GET', 'POST'].\n"
     ]
    }
   ],
   "source": [
    "# Similarly, of the endpoint name have been defined before, we should delete them before create a new one.\n",
    "# serve.delete_endpoint(\"nextword\")\n",
    "serve.create_endpoint(\"nextword\", \"/nextword\", methods=[\"GET\", \"POST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the endpoint with the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.set_traffic(\"nextword\", {\"nextword\": 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can send the request to the server and receive the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Manhattan bridge is a major part'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.post(\"http://127.0.0.1:8000/nextword\", data=\"The Manhattan bridge is a major\")\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3)",
   "language": "python",
   "name": "conda_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
